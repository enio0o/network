
https://www.server-world.info/en/note?os=Debian_11&p=microk8s&f=1




# Configure CA and gitlab

```text
1) На машине cicd разверните gitlab
2) Защитите подключение к gitlab при помощи https
3) Сконфигурируйте gitlab registry для хранения образова docker контейнеров
4) Gitlab должен быть доступен по адресу https://gitlab.company.prof
5) Используйте директорию /opt/gitlab для хранения файлов gitlab
```

## Первым делом нам нужно позаботиться о сертификатах, поэтому следует произвести настройку центра сертификации и сгенерировать сертификаты для gitlab

Идем править конфиг по пути **/etc/ssl/openssl.cnf**

```text
Нас интересуют строки:
dir
copy_extensions
policy 
```

Приведите данные строчки к такому виду:

dir=/etc/ca

copy_extensions=copy

policy=policy_anything

Идем править конфиг по пути **/usr/lib/ssl/misc/CA.pl**

Приведите строку CATOP к данному виду:
CATOP=/etc/ca

Теперь можно произвести настройку центра сертификации

```bash
cd /usr/lib/ssl/misc
```

```bash
CA.pl -newca
```

Скипаем все, кроме Common Name и PEM phase phrase


Теперь сгенерируем сами сертификаты

```bash
cd /usr/lib/ssl/misc
```

Генерим сертификат

```bash
openssl req -new -sha256 -nodes -out newreq.pem -newkey rsa:2048 -keyout newkey.pem -config <(
cat <<-EOF
[req]
default_bits = 2048
prompt = no
default_md = sha256
req_extensions = req_ext
distinguished_name = dn

[ dn ]
C=US
ST=New York
L=Rochester
O=End Point
OU=Testing Domain
CN = gitlab.company.prof

[ req_ext ]
subjectAltName = @alt_names

[ alt_names ]
DNS.1 = gitlab.company.prof
DNS.2 = 10.10.10.100 //тут адрес хоста, где расположен гитлаб
EOF
)
```

Полученные сертификаты подписываем

```bash
./CA.pl -sign
```

### Теперь можно перейти к установке gitlab

Экспортим переменную отвечающую за расположение файлов конфигурации гитлаб

```bash
export GITLAB_HOME=/opt/gitlab
```

Создаем директорию для сертификатов

```bash
mkdir /etc/gitlabcer
```

В данную директорию требуется скопировать сгенеренные нами раннее сертификаты newcert.pem и newkey.pem

Ставим docker

```bash
apt install docker docker.io docker-compose -y
```

Теперь создаем и заполняем файл docker-compose.yml таким образом:

```Также обратите внимание, что гитлаб использует 22 порт, поэтому нужно переместить ssh на альтернативный порт или убрать проброс портов из конфигурации```

```yaml
version: '3.6'
services:
  web:
    image: 'gitlab/gitlab-ee:latest'
    restart: always
    hostname: 'gitlab.company.prof'
    environment:
      GITLAB_OMNIBUS_CONFIG: |
        external_url   'https://gitlab.company.prof'
        letsencrypt['enable'] = false
        registry_external_url  'https://gitlab.company.prof:5050'
        registry_nginx['ssl_certificate'] = "/etc/gitlabcer/newcert.pem"
        registry_nginx['ssl_certificate_key'] = "/etc/gitlabcer/newkey.pem"
        nginx['ssl_certificate'] = "/etc/gitlabcer/newcert.pem"
        nginx['ssl_certificate_key'] = "/etc/gitlabcer/newkey.pem"
        # Add any other gitlab.rb configuration here, each on its own line
    ports:
      - '80:80'
      - '443:443'
      - '5050:5050'
      - '22:22'
    volumes:
      - '/etc/gitlabcer:/etc/gitlabcer'
      - '$GITLAB_HOME/config:/etc/gitlab'
      - '$GITLAB_HOME/logs:/var/log/gitlab'
      - '$GITLAB_HOME/data:/var/opt/gitlab'
    shm_size: '256m'

```

Поднимаем гитлаб

```bash
docker-compose up -d
```

Гитлаб довольно долго стартует, поэтому нужно будет подождать. Как только у контейнера будет состояние healthy - можно идти в веб.

Для того, чтобы получить пароль от админа используйте команду

```bash
docker exec -it gitlab grep 'Password:' /etc/gitlab/initial_root_password
```

*gitlab - имя контейнера

Готово, теперь можно переходить по адресу <https://gitlab.company.prof> по кредам root/пароль_полученный_из_контейнера

## Загружаем архив с приложением

Идем и создаем access token

administrator-hitqaap-accestokens

Нажимаем add token


Заполняем как на картинке и  нажимаем create token

owner все галки tokem

Сохраняем куда-нибудь токен

![Alt text](image-8.png)

После создания репозитория появляется инструкция по загрузке файлов

Следуем этой инструкции, вместо пароля от юзера используем токен

![Alt text](image-9.png)









# Configure gitlab runners, pipelines

```text
Установите два gitlab раннера на ВМ worker1 и worker2
Подключите раннеры к gitlab
Раннеры должны быть сконфигурированы в режиме docker
```

Переходим на машины worker1&worker2 и ставим пакет gitlab-runner

```bash
curl -L "https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh" | bash
```

```bash
apt install gitlab-runner
```

Конфигурим раннеры

a) Переходим: **Settings > CI / CD > Runners > Expand > Create gitlab runner**

1) Выбираем OS = Linux
2) Ставим галочку Run untagged jobs
3) Создаем раннер

b) Следуем инструкции и вводим команду:

```bash
gitlab-runner register  --url https://gitlab.company.prof  --token your_token_from_gitlab
```
1) В качестве executor пишем docker
2) В качестве image пишем alpine

После регистрации раннера следует поправить конфигурацию под наши нужды:

```bash
vim /etc/gitlab-runner/config.toml
```

Нас интересуют строки:

```text
url
executor
image
privileged
volumes
network_mode
```

```ini
concurrent = 1
check_interval = 0
shutdown_timeout = 0

[session_server]
  session_timeout = 1800

[[runners]]
  name = "debian"
  url = "https://gitlab.company.prof"
  id = 2
  token = "your_token_from_gitlab"
  token_obtained_at = 2023-09-12T12:53:14Z
  token_expires_at = 0001-01-01T00:00:00Z
  executor = "docker"
  [runners.cache]
    MaxUploadedArchiveSize = 0
  [runners.docker]
    tls_verify = false
    image = "alpine:latest"
    privileged = true 
    disable_entrypoint_overwrite = false
    oom_kill_disable = false
    disable_cache = false
    volumes = ["/cache", "/var/run/docker.sock:/var/run/docker.sock", "/etc/ssl/ca.crt:/etc/gitlab-runner/certs/ca.crt:ro"]
    network_mode = 'host'
    shm_size = 0
```

После правки конфига надо ребутнуть сервис раннера и отследить его доступность в интерфейсе гитлаба. Если зеленый - значит раннер приконнектился и с ним можно работать.

```(Повторить все действия выше на worker2)```



### Install microk8s cluster

1) Ставим snap
```
su root
apt update
apt install snapd
```
2) Устанавливаем microk8s
```
sudo snap install microk8s --classic --channel=1.28
```
3) Добавляем пользователя в группу и назначем владельцем (Делать не из под рута)
```
sudo usermod -a -G microk8s $USER
sudo chown -f -R $USER ~/.kube
```
4) Проверяем, что ноды доступны и работоспособны
```
microk8s kubectl get nodes
```

//чтобы использовать просто kubectl, вместо microk8s kubectl используйте алиасы
```
alias kubectl='microk8s kubectl'
```

### Install argocd
1) Включаем днс
```
microk8s enable dns && microk8s stop && microk8s start
```
2) Ставим argocd
```
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
```
3) Для доступа к аргосд по адресу будем использовать балансировщик metallb
```
microk8s enable metallb
```
При запросе пула адресов вводим свободные адреса в нашей локальной сети, например:
```
Enter each IP address range delimited by comma (e.g. '10.64.140.43-10.64.140.49,192.168.0.105-192.168.0.111'): 10.0.10.120-10.0.10.130
```
4) Редактируем дефолтный адвертисмент
```
kubectl edit L2Advertisement default-advertise-all-pools -n metallb-system
```
В нем добавляем default-addresspoll:
```
spec:
  ipAddressPools:
    - default-addresspool
```
5) Готово, чтобы узнать адрес аргосд посмотрите
```
kubectl get svc -n argocd | grep argocd-server
```


### Install postgresql

1) Apply manifests

a) deployment.yaml
```
# Kubernetes API version
apiVersion: apps/v1
# Deployment object
kind: Deployment
metadata:
  # The name of the Deployment
  name: postgres
spec:
  # Replicas for this Deployment
  replicas: 1
  selector:
    # labels the pods
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        # The label the pods created from the pod template should have
        app: postgres
    spec:
      containers:
        # The container name to execute pods
        - name: postgres
          # pull postgresimage from docker hub
          image: postgres
          ports:
            # Assign ports to expose container
            - containerPort: 5432
          envFrom:
            # Load the environment variables/PostgresSQL credentials
            - configMapRef:
                # This should be the ConfigMap name created ealier
                name: db-secret-credentials
          volumeMounts:
            # The volume mounts  for the container
            - mountPath: /var/lib/postgres/data
              name: db-data
      # Volumes attached to the pod
      volumes:
        - name: db-data
          persistentVolumeClaim:
            # reference the PersistentVolumeClaim
            claimName: db-persistent-volume-claim
```
b) configmap.yaml
```
apiVersion: v1
# Kind for kubernets ConfigMap
kind: ConfigMap
metadata:
  # Name your ConfigMap
  name: db-secret-credentials
  labels:
    app: postgres
data:
  # User DB
  POSTGRES_DB: postgres_DB
  # Db user
  POSTGRES_USER: postgres
  # Db password
  POSTGRES_PASSWORD: postgres
```
c) pv.yaml
```
apiVersion: v1
# Kind for volume chain
kind: PersistentVolume
metadata:
  # Name the persistent chain
  name: postgresdb-persistent-volume
  # Labels for identifying PV
  labels:
    type: local
    app: postgres
spec:
  storageClassName: manual
  capacity:
    # PV Storage capacity
    storage: 8Gi
  # A db can write and read from volumes to multiple pods
  accessModes:
    - ReadWriteMany
  # Specify the path to persistent the volumes  
  hostPath:
    path: "/data/db"
```
d) pvc.yaml
```
apiVersion: v1
# define a resource for volume chain
kind: PersistentVolumeClaim
metadata:
  # Name the volume chain
  name: db-persistent-volume-claim
spec:
  storageClassName: manual
  accessModes:
    # Allow ReadWrite to multiple pods
    - ReadWriteMany
  # PVC requesting resources
  resources:
    requests:
      # the PVC storage
      storage: 8Gi
```
e) service.yaml
```
apiVersion: v1
# Kind for service
kind: Service
metadata:
  # Name your service
  name: postgres
  labels:
    app: postgres
spec:
  # Choose how to expose your service
  type: NodePort
  ports:
    # The port number to expose the service
    - port: 5432
  # Pod to route service traffic  
  selector:
    app: postgres
```

## Создайте директорию postgresql, поместите туда все манифесты указанные выше и примените команду:
```
kubectl apply -f postgresql/
```




Для настройки GitLab Container Registry, который позволяет хранить образы Docker контейнеров, выполните следующие шаги. Предполагается, что у вас уже установлен GitLab.

Шаг 1: Редактирование конфигурационного файла GitLab
Откройте файл конфигурации GitLab, обычно он находится в /etc/gitlab/gitlab.rb.

Добавьте или отредактируйте следующие строки в файле gitlab.rb, чтобы включить Container Registry:

ruby
Copy code
registry_external_url 'https://registry.gitlab.company.prof'

gitlab_rails['registry_enabled'] = true
gitlab_rails['registry_host'] = "registry.gitlab.company.prof"
gitlab_rails['registry_port'] = "5050"
gitlab_rails['registry_path'] = "/var/opt/gitlab/gitlab-rails/shared/registry"
В этом примере:

registry_external_url — URL, по которому будет доступен Registry.
registry_host и registry_port — хост и порт для Registry.
registry_path — путь, где будут храниться данные Registry.
Шаг 2: Применение изменений
После редактирования конфигурации выполните следующие команды, чтобы применить изменения:

bash
Copy code
sudo gitlab-ctl reconfigure
sudo gitlab-ctl restart
Это перенастроит и перезапустит GitLab с включенным Container Registry.

Шаг 3: Настройка безопасности
Если вы используете HTTPS (что рекомендуется для безопасности), убедитесь, что у вас есть соответствующий SSL-сертификат для registry.gitlab.company.prof. Если сертификат подписан удостоверяющим центром (CA), его необходимо установить в соответствии с документацией GitLab. Это может потребовать добавления путей к сертификату и ключу в gitlab.rb.

Шаг 4: Настройка DNS
Убедитесь, что DNS для registry.gitlab.company.prof настроен правильно и указывает на ваш сервер GitLab.

Шаг 5: Использование Registry
После настройки и запуска GitLab Registry, вы сможете использовать его для хранения Docker образов. Для работы с Registry:

Войдите в Registry через Docker:
bash
Copy code
docker login registry.gitlab.company.prof
Используйте команды docker push и docker pull для загрузки и скачивания образов из вашего Registry.
Дополнительная информация
Проверьте права доступа к каталогу, указанному в registry_path.
Убедитесь, что порты, используемые Registry (например, 5050), открыты в брандмауэре вашего сервера.
Подробные инструкции и дополнительные настройки можно найти в официальной документации GitLab.
Важно помнить, что настройка сервиса, такого как GitLab Registry, требует тщательного внимания к деталям конфигурации и безопасности. Убедитесь, что вы следуете лучшим практикам и регулярно обновляете вашу систему и приложения.




Чтобы создать локального пользователя в GitLab и предоставить ему доступ ко всем проектам, вам нужно выполнить следующие шаги:

Шаг 1: Вход в GitLab как Администратор
Откройте веб-интерфейс GitLab.
Войдите в систему, используя учетную запись администратора. Обычно это учетная запись root, если вы только что установили GitLab.
Шаг 2: Создание Пользователя
Перейдите в Admin Area (обычно доступна по клику на значок шестеренки в верхнем меню).
В меню слева выберите Users.
Нажмите на кнопку New User в верхней части страницы.
Заполните необходимые поля:
Name: Введите имя пользователя, например "Stasyan".
Username: Введите имя пользователя для входа, например "stasyan".
Email: Укажите действующий адрес электронной почты.
Password: Установите пароль (в вашем случае P@ssw0rd).
Убедитесь, что выбрана опция Can Create Group и Can Create Project, если вы хотите, чтобы этот пользователь имел возможность создавать группы и проекты.
Нажмите кнопку Create user.
Шаг 3: Назначение Прав Доступа
Чтобы пользователь stasyan мог видеть все проекты:

Для существующих проектов: Вам нужно будет добавить пользователя stasyan в каждый проект и предоставить ему соответствующий уровень доступа (например, как Reporter, Developer, Maintainer и т.д.).
Перейдите в каждый проект, затем в Settings > Members, и добавьте stasyan с необходимым уровнем доступа.
Для будущих проектов: Можно настроить группы так, чтобы новые проекты автоматически предоставляли доступ stasyan.
Создайте группу, в которой stasyan будет администратором или членом с необходимыми правами.
При создании новых проектов, убедитесь, что они принадлежат этой группе.
Шаг 4: Проверка
После создания пользователя и настройки прав доступа, войдите в GitLab под учетной записью stasyan, чтобы убедиться, что все настроено корректно.
Дополнительные Замечания
Управление доступом к проектам в GitLab лучше всего организовывать с помощью групп. Это позволяет централизованно управлять доступом к множеству проектов.
Будьте осторожны с уровнями доступа, которые вы предоставляете пользователям, чтобы избежать несанкционированного изменения или доступа к конфиденциальной информации.
Это базовая настройка для создания пользователя и настройки доступа. Зависимо от вашей конкретной ситуации и требований к безопасности, вам может потребоваться выполнить дополнительные шаги.







Установка MicroK8s на всех узлах:

На каждом узле (K8S-MASTER, K8S-WORKER1-HQ, K8S-WORKER2-HQ) установите MicroK8s. Вы можете сделать это, используя команду:
bash
Copy code
sudo snap install microk8s --classic
После установки убедитесь, что MicroK8s активен и работает:
bash
Copy code
sudo microk8s status --wait-ready
Настройка мастера (на K8S-MASTER):

Включите нужные службы на мастере:
bash
Copy code
sudo microk8s enable dns dashboard storage
Для добавления рабочих узлов в кластер, сначала получите токен соединения на мастере:
bash
Copy code
sudo microk8s add-node
Эта команда выведет команду, которую нужно будет выполнить на рабочих узлах, чтобы присоединить их к кластеру.
Присоединение рабочих узлов (на K8S-WORKER1-HQ и K8S-WORKER2-HQ):

Выполните команду, полученную на шаге 2, на обоих рабочих узлах. Это выглядит примерно так:
bash
Copy code
sudo microk8s join <master-node-ip>:25000/<token>
Проверьте, что рабочие узлы успешно присоединились к кластеру, выполнив на мастере:
bash
Copy code
sudo microk8s kubectl get nodes
Дополнительные настройки:

В зависимости от ваших требований, вы можете включить дополнительные службы на мастере или рабочих узлах, такие как ingress, metrics-server и другие.
Не забудьте настроить сетевую политику и правила брандмауэра, чтобы обеспечить связь между узлами.
Проверка кластера:

Используйте kubectl, чтобы управлять кластером и проверить его состояние. Например, для просмотра запущенных подов:
bash
Copy code
sudo microk8s kubectl get pods --all-namespaces
Убедитесь, что следуете лучшим практикам безопасности и управления кластерами Kubernetes, особенно если планируете использовать этот кластер в производственной среде


Для конфигурации баз данных PostgreSQL, MongoDB и Redis в вашем MicroK8s кластере, а также настройки liveness и readiness probes для приложения, выполните следующие шаги:

Конфигурация баз данных:
1. PostgreSQL:
Установка:

Используйте Helm для установки PostgreSQL. Если Helm еще не установлен, активируйте его на вашем MicroK8s кластере:
bash
Copy code
sudo microk8s enable helm3
Установите чарт PostgreSQL:
bash
Copy code
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install my-postgresql bitnami/postgresql
Конфигурация:

Можете указать параметры конфигурации в файле values.yaml или передать их непосредственно в команде helm install. Например, для установки пароля:
bash
Copy code
helm install my-postgresql bitnami/postgresql --set postgresqlPassword=mysecretpassword
2. MongoDB:
Установка:

Аналогично, установите MongoDB с помощью Helm:
bash
Copy code
helm install my-mongodb bitnami/mongodb
Конфигурация:

Укажите необходимые параметры конфигурации при установке или в файле values.yaml.
3. Redis:
Установка:

Используйте Helm для установки Redis:
bash
Copy code
helm install my-redis bitnami/redis
Конфигурация:

Как и в предыдущих случаях, настройте Redis, используя параметры командной строки или файл values.yaml.
Настройка liveness и readiness probes:
Liveness и Readiness Probes:
Liveness Probe определяет, когда перезапустить контейнер. Например, если приложение застряло и не может обрабатывать запросы.

Readiness Probe определяет, когда контейнер готов к обработке запросов.

Пример конфигурации:



В вашем Kubernetes Deployment YAML, добавьте liveness и readiness probes к конфигурации контейнера. Например, для HTTP-приложения:
yaml
Copy code
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 15
  timeoutSeconds: 2
  periodSeconds: 5

readinessProbe:
  httpGet:
    path: /readiness
    port: 8080
  initialDelaySeconds: 5
  timeoutSeconds: 1
  periodSeconds: 5
Адаптируйте пути (/healthz, /readiness), порты и другие параметры в соответствии с вашим приложением.
Убедитесь, что ваши конфигурации соответствуют требованиям вашего приложения и следуют лучшим практикам Kubernetes. Также рекомендуется регулярно обновлять и поддерживать ваши Helm чарты и Kubernetes конфигурации для обеспечения безопасности и надежности вашего кластера.


Включение аддонов Prometheus и Grafana в MicroK8s:

Откройте терминал и выполните следующие команды:
bash
Copy code
microk8s enable prometheus
microk8s enable grafana
Эти команды установят и настроят Prometheus и Grafana в вашем кластере MicroK8s.
Проверка статуса:

Проверьте статус установленных служб:
bash
Copy code
microk8s status
Убедитесь, что Prometheus и Grafana запущены и работают корректно.
Шаг 2: Настройка доступа к Grafana
Настройка Ingress для Grafana:

Для настройки доступа к Grafana по URL https://grafana.company.prof вам необходимо создать Ingress-ресурс.
Вам потребуется записать YAML-файл для Ingress, который перенаправит запросы на Grafana. Пример конфигурации Ingress:
yaml
Copy code
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: grafana.company.prof
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: grafana
            port:
              number: 3000
Настройка SSL:

Если у вас есть SSL-сертификат для вашего домена, вы должны указать его в настройках Ingress.
Если используете Let's Encrypt, можете воспользоваться cert-manager для автоматической установки сертификатов.
Шаг 3: Создание Дашбордов в Grafana
Доступ к Grafana:

После настройки Ingress, откройте https://grafana.company.prof в браузере.
Используйте учетные данные по умолчанию (обычно admin/admin) и измените пароль на P@ssw0rd.
Создание дашборда Pods:

В Grafana, создайте новый дашборд и добавьте на него панель (Panel).
В качестве источника данных выберите Prometheus.
Настройте запросы Prometheus для получения данных о CPU и ОЗУ всех подов. Пример запроса для CPU: sum(rate(container_cpu_usage_seconds_total{namespace="default"}[5m])) by (pod).
Повторите аналогичные шаги для ОЗУ.
Создание дашборда Cluster:

Создайте еще один дашборд для общего мониторинга кластера.
Добавьте панели для отображения общей нагрузки CPU и ОЗУ кластера. Используйте запросы агрегации в Prometheus.
Шаг 4: Сохранение и использование дашбордов
После настройки панелей не забудьте сохранить дашборды и регулярно их просматривать для мониторинга состояния кластера.
Дополнительные замечания:
Процесс настройки Ingress и SSL может отличаться в зависимости от вашей среды и конкретного Ingress Controller.
Для создания более сложных запросов в Prometheus и настройки панелей Grafana может потребов

Войдите в GitLab: Откройте GitLab и войдите в свою учетную запись.

Создание нового проекта:

Нажмите на кнопку "New project" или "Create a project".
Выберите "Blank project".
В поле "Project name" введите hitech2023-app.
(Необязательно) Добавьте описание проекта.
Выберите уровень видимости проекта (например, Private).
Нажмите на кнопку "Create project".
Шаг 2: Загрузка исходного кода приложения
Клонирование репозитория локально:

Используйте команду git clone, чтобы клонировать пустой репозиторий на свой компьютер.
bash
Copy code
git clone <url-репозитория>
Добавление исходного кода:

Переместите исходный код вашего приложения в клонированный репозиторий.
Фиксация изменений в репозитории:

Используйте команды git add, git commit, и git push, чтобы добавить и отправить исходный код в репозиторий GitLab.
bash
Copy code
git add .
git commit -m "Initial commit with application source code"
git push -u origin master
Шаг 3: Создание директории для манифестов Kubernetes
Создание директории Kubernetes:

В вашем локальном репозитории создайте директорию kubernetes.
bash
Copy code
mkdir kubernetes
Добавление манифестов Kubernetes:

Разместите все манифесты Kubernetes, которые вы хотите использовать, в этой директории.
Фиксация и отправка манифестов:

Используйте git add, git commit, и git push, чтобы добавить манифесты Kubernetes в репозиторий.
bash
Copy code
git add kubernetes/
git commit -m "Add Kubernetes manifests"
git push
Шаг 4: Настройка ArgoCD для синхронизации
Настройка ArgoCD:

Настройте ArgoCD для отслеживания изменений в директории kubernetes вашего репозитория.
Создайте приложение в ArgoCD, указав путь к директории kubernetes в настройках источника.
Синхронизация:

После настройки ArgoCD будет автоматически синхронизировать изменения в директории kubernetes с вашим кластером Kubernetes.
Эти инструкции предоставят вам основу для создания репозитория в GitLab, добавления исходного кода приложения, организации манифестов Kubernetes и их синхронизации с кластером Kubernetes через ArgoCD. Убедитесь, что у вас есть все необходимые доступы и права на выполнение этих операций в вашей среде.

Для настройки CI/CD в GitLab, которая соответствует вашим требованиям, вам нужно создать файл .gitlab-ci.yml в корне вашего репозитория. Этот файл будет определять различные этапы и задачи в вашем CI/CD процессе. Ниже приведен пример конфигурации, которая удовлетворяет указанным требованиям:

Файл .gitlab-ci.yml
yaml
Copy code
stages:
  - lint
  - build
  - deploy

variables:
  DOCKER_HOST: tcp://docker:2375/
  DOCKER_DRIVER: overlay2
  IMAGE_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA

before_script:
  - docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY

lint-backend:
  stage: lint
  script:
    - pip install pylint
    - pylint app-backend/*
  only:
    changes:
      - app-backend/**/*

build-backend:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  script:
    - docker build -t $IMAGE_TAG-backend app-backend/.
  only:
    changes:
      - app-backend/**/*
  tags:
    - docker

build-frontend:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  script:
    - docker build -t $IMAGE_TAG-frontend app-frontend/.
  only:
    changes:
      - app-frontend/**/*
  tags:
    - docker

deploy:
  stage: deploy
  script:
    - docker push $IMAGE_TAG-backend
    - docker push $IMAGE_TAG-frontend
  only:
    - master
Объяснение:
Стадии: Определены три стадии: lint, build, и deploy.
Переменные: DOCKER_HOST и DOCKER_DRIVER нужны для работы с Docker. IMAGE_TAG создает метку для образов Docker.
Linting бэкенда: Задача lint-backend запускается на стадии lint и проверяет синтаксис кода в директории app-backend с помощью pylint.
Сборка бэкенда: Задача build-backend собирает Docker образ для бэкенда, если были изменения в app-backend.
Сборка фронтенда: Аналогично, build-frontend собирает Docker образ для фронтенда, если были изменения в app-frontend.
Отправка контейнеров: На стадии deploy, образы отправляются в GitLab Container Registry. Эта стадия срабатывает только при изменениях в ветке master.
Важные Замечания:
Docker и Docker-in-Docker: Эта конфигурация использует Docker-in-Docker (docker:dind), что требует соответствующей настройки GitLab Runner.

Безопасность: Убедитесь, что ваш GitLab Runner безопасно настроен, особенно при использовании Docker-in-Docker.

pylint: В примере предполагается, что pylint может проверить весь код в директории app-backend. Вам может потребоваться настроить pylint для вашего проекта.

Изменения только в определенных директориях: С помощью ключа only: changes: задачи будут запускаться только при изменениях в указанных директориях.

Настройка Dockerfile: Убедитесь, что у вас есть соответствующие Dockerfile в директориях app-backend и app-frontend для сборки образов.

Эта конфигурация является базовой и может потребовать дополнительной настройки в зависимости от специфики вашего проекта и среды.

Создание Ansible-роли для настройки АРМ пользователей подразумевает несколько ключевых шагов. Я подробно опишу, как создать роль client-deploy-role, которая устанавливает указанное ПО, конфигурирует доступ к sudo и управляет ярлыками на рабочем столе.

Шаг 1: Создание структуры роли
Создайте каталог роли:

bash
Copy code
mkdir -p /etc/ansible/roles/client-deploy-role/{tasks,handlers,templates,files}
Структура роли:

tasks: Основные задачи роли.
handlers: Обработчики, которые используются для перезапуска служб и других подобных операций.
templates: Шаблоны для файлов конфигурации.
files: Статические файлы, такие как ярлыки для рабочего стола.
Шаг 2: Определение задач
Файл задач (tasks/main.yml):
Создайте файл /etc/ansible/roles/client-deploy-role/tasks/main.yml со следующим содержанием:

yaml
Copy code
---
- name: Установка Firefox
  package:
    name: firefox
    state: present

- name: Установка Zabbix-agent
  package:
    name: zabbix-agent
    state: present

- name: Установка Midnight-commander
  package:
    name: mc
    state: present

- name: Настройка доступа к sudo для группы group1
  lineinfile:
    path: /etc/sudoers
    state: present
    regexp: '^%group1'
    line: '%group1 ALL=(ALL:ALL) ALL'
    validate: 'visudo -cf %s'

- name: Настройка ярлыков на рабочем столе
  copy:
    src: firefox.desktop
    dest: /home/{{ item }}/Desktop/
    mode: '0644'
  loop: "{{ ansible_users }}"
  when: ansible_users is defined

- name: Удаление всех других ярлыков с рабочего стола
  find:
    paths: "/home/{{ item }}/Desktop/"
    patterns: '*.desktop'
    excludes: 'firefox.desktop'
  loop: "{{ ansible_users }}"
  register: desktop_files

- name: Удаление лишних ярлыков
  file:
    path: "{{ item[1].path }}"
    state: absent
  loop: "{{ desktop_files.results }}"
  when: desktop_files is defined and desktop_files.results | length > 0

Шаг 3: Создание файлов ярлыков
Добавьте ярлык Firefox в каталог files:
Создайте файл firefox.desktop в каталоге /etc/ansible/roles/client-deploy-role/files.
Добавьте в этот файл стандартное содержимое файла .desktop для Firefox.
Шаг 4: Создание плейбука
Создание плейбука (/etc/ansible/client_install.yml):

yaml
Copy code
---
- hosts: all
  become: yes
  roles:
    - client-deploy-role
Этот плейбук будет применять роль client-deploy-role к указанным хостам.

Шаг 5: Запуск плейбука
Выполните плейбук командой:

bash
Copy code
ansible-playbook /etc/ansible/client_install.yml
Важные замечания:
В примере предполагается, что у вас есть список пользователей (ansible_users), для которых нужно настроить ярлыки. Этот список может быть определен в вашем инвентаре Ansible




